\documentclass{acm_proc_article-sp}
\usepackage{url}
\usepackage{algpseudocode}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{epsfig}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\begin{document}

\title{Cross Language Similar Sentence Detection}

\numberofauthors{3}

\author{
\alignauthor Sarah E. Weissman\\
       \affaddr{University of Maryland}\\
       \affaddr{College of Information Studies}\\
       \email{sew@umd.edu}
}

\date{November 24, 2013}

\maketitle
\begin{abstract}
\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.3}{Information Storage and Retrieval}[Information Search and Retrieval]

\terms{Minhash, MapReduce}

\section{Introduction}

\section{Related Work}

\section{Cross-lingual Similarity at the Sentence Level}

We assume a model where a document is represented by a set of features. In the single language case the set of features is typically taken directly from the text of a document, either breaking up text on non-word boundaries (a ``bag of words'') or by apply a sliding window technique to create a set of equal-length document ``shingles'' (ref?). In cross-language similarity matching, the set of features for a document can no longer be extracted directly from the text, since there is no reason to assume that documents across languages share text features. One approach to the problem would be to pick a larget language $E$ and use machine translation to convert all documents into $E$ then proceed as in the single language case. The problem with this approach is that most language models are probabilistic, resulting in many possible translations for each document. Also, there is evidence that translation approaches do not significantly out perform CLIR techniques, which are generally less computationally expensive than machine translation techniques (REF). Finally, in order to build a machine translation model, it is often necessary to produce sets of parallel text, for which one would need a similarity measure, so it does not always make sense for document similarity matching techniques to depend on accurate translation models already existing.

For the purposes of this work, our input documents are sentence, which allows us finer granularity in information retrieval than at the article level. Although the sentence level is somewhat arbitrary, it is generally a reliable way to break up documents, since most documents are composed in sentences, and sentences can be reliably extracted from text using regular expression matching for many languages. We use the signature based minhashing technique. In order to achieve cross language comparison we map each document word set into our target language by using a word-based language model for which we know $p(e|f)$, the conditional probability of a target language word $e$ being the translation of a foreign language word $e$. For each foreign word sentence we map the sentence into a set of words $S_F$. For each $f \in S_F$ we sample from the set of possible translations according to the distribution $p(f|e)$. We repeat this process $N$ times to get $N$ possible word sets (for some fixed $N$). These ``translated'' word sets are then each used as input into the minhash signature generation process. Although this approach is simplistic, it requires only a single pass over the input document set to process \emph{document} $\rightarrow$ \emph{sentence} $\rightarrow$ \emph{word set} $\rightarrow$ \emph{set of translated word sets}.

\section{Locality \\ Sensitive Hashing}

In order to measure document similarity we use a common LSH technique known as MinHash (\cite{broder:resemblance}). A minhash signature on a text document is calculated using a parametrized family of hash functions $F_i$, $1 \le i \le N$. For each input feature set $S$ (in our case a set of words or translated words) a vector $\{min_{s \in S}(F_i(s)\}$ of minimum hashes over the hash family is produced. The signature on a document $d$ is represented as a vector of $K$ minhashes, chosen from the set $\{min_{s \in S}(F_i(s)\}$ of minimum hashes. In order to minimize false negatives we apply a technique sometimes known as ``banding'' \cite{ullman:massive} where multiple signatures are produced for each input document.

\subsection{Implementation Details}

Our implementation is built on top of the Apache Hadoop MapReduce framework, using utilities from the Cloud9 (\url{https://github.com/lintool/Cloud9}), Ivory  and WikiClean (\url{https://github.com/lintool/wikiclean/}) libraries for parsing Wikipedia page text from the Wikipedia XML dump format. An open source implementation is available on Github (\url{https://github.com/seweissman/wikiduper}).

\section{Experiments and Discussion}



\begin{figure}
\centering
\includegraphics[width=3.5in, keepaspectratio = true]{classifyscoreswikinew.pdf}
\caption{Histogram of Hand Labeled Clusters by Template Heuristic Score.}
\label{heuristic}
\end{figure}


\section{Conclusions and Future Work}

\bibliographystyle{abbrv}
\bibliography{article-draft}  % sigproc.bib is the name of the Bibliography in this case

\balancecolumns
\end{document}
